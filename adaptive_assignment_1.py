# -*- coding: utf-8 -*-
"""adaptive assignment_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-NZ4mCgNUuQnyTFJf_bywzi2jtZeXe1t
"""

import numpy as np

# Define sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Initialize network parameters
input_nodes = 5
hidden_nodes = 10
output_nodes = 3
learning_rate = 0.1

np.random.seed(0)
weights_input_hidden = np.ones((input_nodes, hidden_nodes))
weights_hidden_output = np.ones((hidden_nodes, output_nodes))
bias_hidden = np.ones((1, hidden_nodes))
bias_output = np.ones((1, output_nodes))

# Function to perform feedforward
def feedforward(X):
    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden
    hidden_output = sigmoid(hidden_input)
    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output
    final_output = sigmoid(final_input)
    return hidden_output, final_output

# Function to compute sum-of-squares loss
def compute_loss(y, t):
    return 0.5 * np.sum((y - t) ** 2)

# Function to perform backpropagation
def backpropagation(X, hidden_output, output, target):
    global weights_input_hidden, weights_hidden_output, bias_hidden, bias_output

    output_error = output - target
    output_delta = output_error * sigmoid_derivative(output)

    hidden_error = np.dot(output_delta, weights_hidden_output.T)
    hidden_delta = hidden_error * sigmoid_derivative(hidden_output)

    weights_hidden_output -= learning_rate * np.dot(hidden_output.T, output_delta)
    weights_input_hidden -= learning_rate * np.dot(X.T, hidden_delta)
    bias_output -= learning_rate * output_delta
    bias_hidden -= learning_rate * hidden_delta

# Read input from user
data = [float(input()) for _ in range(8)]
inputs = np.array(data[:5]).reshape(1, -1)
targets = np.array(data[5:]).reshape(1, -1)

# Compute initial loss
hidden_output, output = feedforward(inputs)
initial_loss = compute_loss(output, targets)

# Compute initial loss
hidden_output, output = feedforward(inputs)
initial_loss = compute_loss(output, targets)

# Perform one backpropagation step
backpropagation(inputs, hidden_output, output, targets)

# Compute loss after training
hidden_output, output = feedforward(inputs)
final_loss = compute_loss(output, targets)

# Print results rounded to 4 decimal places
print(f"{initial_loss:.4f}")
print(f"{final_loss:.4f}")

